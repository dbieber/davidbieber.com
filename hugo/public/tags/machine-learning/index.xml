<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on David Bieber</title>
    <link>https://davidbieber.com/tags/machine-learning/</link>
    <description>Recent content in machine-learning on David Bieber</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://davidbieber.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dealing with Giant Group Chats</title>
      <link>https://davidbieber.com/snippets/2022-07-05-giant-group-chats/</link>
      <pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2022-07-05-giant-group-chats/</guid>
      
      <description>&lt;p&gt;Sometimes &amp;ndash; usually because of Alex K Chen &amp;ndash; I am added to a giant group chat (GGC).
Today it was a 200+ person highly active chat with folks talking about AI programming tools, aging, and movies.
I love being added to such a chat. It makes me feel thought of, and my opinions respected. So, do keep adding me to GGCs.
However, with being added to a GGC comes some struggles. I haven&amp;rsquo;t fully figured out how to handle a GGC just yet.&lt;/p&gt;
&lt;p&gt;I think some people handle these situations in a more natural way than myself. Here&amp;rsquo;s what I imagine. They&amp;rsquo;re doing something else on the computer, messages are open in the background or in a side window. If they have a conversation-adjacent thought, they share it. If they see a message that looks interesting, they read it. Otherwise, they do their other activities and get on with their life.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If they have a conversation-adjacent thought, they share it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This part is Struggle #1.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If they see a message that looks interesting, they read it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This part is Struggle #2.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;struggle-1-sharing-thoughts-casually-in-a-200-person-setting&#34;&gt;Struggle #1: Sharing thoughts casually in a 200+ person setting&lt;/h3&gt;
&lt;p&gt;Why is this a struggle? I think slowly. I edit. I rethink relevance as the conversation moves away.
The hypothetical person who handles GCCs as described above just types and hits enter.
I think about the permanence of putting something on the internet.
I think about confidentiality of work-adjacent stuff.
I think about whether what I&amp;rsquo;m saying is true, and about how true it is.&lt;/p&gt;
&lt;h3 id=&#34;struggle-2-staying-abreast-of-200-messages-per-hour&#34;&gt;Struggle #2: Staying abreast of 200 messages per hour&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t want to read all these messages.
They glimmer and my attention is drawn to them, but that doesn&amp;rsquo;t reflect my conscious decision making.
I&amp;rsquo;d much rather read a summary of the messages or a digest of the topics discussed once a week or so.
Then, only if a topic of interest was discussed, I might want to dive deeper and read the individual messages, and
contemplate leaving a reply.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;tooling-for-struggle-1&#34;&gt;Tooling for Struggle #1&lt;/h3&gt;
&lt;p&gt;tl;dr Draft messages outside of conversation UI. Use &amp;ldquo;approve for dealyed send&amp;rdquo; model for sending messages.&lt;/p&gt;
&lt;p&gt;Struggle #1 is a personal psychological struggle.
It isn&amp;rsquo;t first and foremost something for which tooling seems relevant.
If I&amp;rsquo;m not comfortable sharing something with 200 people in a chat, why would tooling change that?
Well, as I&amp;rsquo;ve learned from adding 
&lt;a href=&#34;https://davidbieber.com/post/2022-01-08-new-messager-setup/&#34;&gt;Messager in Go Note Go&lt;/a&gt; tooling can actually make a huge difference in this type of space.
If instead of feeling like I&amp;rsquo;m sending a message to a massive group, I instead feel like I&amp;rsquo;m writing a draft of my thoughts, something I&amp;rsquo;ll maybe send later but maybe not, suddenly it becomes so much easier to write.
And then, once it&amp;rsquo;s written, it also becomes much easier to send.
This is the same lesson I&amp;rsquo;ve learned over and over with snippets since I initially decided to set up this system as 
&lt;a href=&#34;https://davidbieber.com/snippets/2019-12-30-writing-for-no-audience/&#34;&gt;writing for no audience&lt;/a&gt; back in 2019.&lt;/p&gt;
&lt;p&gt;So, what would the tooling look like for dealing with Struggle #1 in GGCs?
I would click a message to start drafting a response,
it would copy the message and context into an editor &amp;ndash; Roam Research, say &amp;ndash; and maybe even stick in the template for writing a snippet.
Then, completely outside the context of the group thread I could compose my thoughts.
If I decide to share it back to the group, great.
If I decide to instead post that thought as a snippet, great.
And if it remains relegated to my notes forever and never sees the light of day, that&amp;rsquo;s a fine outcome too.&lt;/p&gt;
&lt;p&gt;[Aside: The acceptability of that outcome is what makes the writing easier. The very possibility of that outcome diminishes the possibility of that outcome by making the other outcomes more likely. It&amp;rsquo;s the opposite of a self-fulfilling prophecy. A self-fulfilling prophesy is an expectation whose existence leads to its own fulfillment; this is an expectation whose very existence works to prevent its fulfillment.]&lt;/p&gt;
&lt;p&gt;One of the lessons I learned from my Go Note Go messager-queue was the value of the &amp;ldquo;approve for delayed send&amp;rdquo; model.
Psychologically it was easier for me to mark a message as approved for sending, and have an automated system later send the message on my behalf once it was approved, rather than directly sending it myself.
Marking something as approved for sending (1) admits the possibility that I make further revisions to the message, and (2) internally relieves myself of some of the responsibility of sending the message. Of course, I am still fully responsible for the message and I accept this responsibility, but nevertheless that perception of decreased responsibility is a relaxing feeling that makes message sending easier.
It also (3) prevents me from actively waiting for replies to come it, a behavior I&amp;rsquo;d like to avoid. It does this because my messages go out without my awareness in the moment, so I cannot sit refreshing or watching for new messages. This way I don&amp;rsquo;t need to expend effort diverting my attention away from waiting for messages; my own unawareness has taken care of this for me.
So, having this &amp;ldquo;approve for delayed send&amp;rdquo; model for dealing with GGCs would be appreciated.&lt;/p&gt;
&lt;h3 id=&#34;tooling-for-struggle-2&#34;&gt;Tooling for Struggle #2&lt;/h3&gt;
&lt;p&gt;Struggle #2 has psychological components, but is also more directly about capabilities.&lt;/p&gt;
&lt;p&gt;The psychological bit:
Keeping up with 200 messages per hour isn&amp;rsquo;t even desirable. Why? It&amp;rsquo;s not that it&amp;rsquo;s a lot of noise &amp;ndash; though sometimes there is a lot of noise.
But even when there&amp;rsquo;s a lot of high quality interesting fun messages, it&amp;rsquo;s not stuff I&amp;rsquo;m actively looking to read or learn about or people that I&amp;rsquo;m choosing to keep in touch with. Instead, it&amp;rsquo;s whatever the group collectively has steered toward.
I&amp;rsquo;d rather be more intentional about what I read and discuss, rather than being at the whims of 200+ people, many of whom I haven&amp;rsquo;t met.
However, there is tension between that desire for intentionality and the desire to hear folks&#39; interesting ideas and share my own.
Adding to the tension further is a natural inclination toward reading messages as they come in, because they&amp;rsquo;re attention-grabbing even if they aren&amp;rsquo;t where I&amp;rsquo;d like to place my focus.&lt;/p&gt;
&lt;p&gt;The capabilities bit:
Even if I decided I did want to stay abreast of the conversation, there&amp;rsquo;s simply too much to reasonably process.
People are discussing topics about which I have a lot to learn.
I don&amp;rsquo;t have the bandwidth to think deeply &amp;ndash; or even shallowly &amp;ndash; about what folks are saying, while also maintaining normal human functions.&lt;/p&gt;
&lt;p&gt;So, what tooling can help?&lt;/p&gt;
&lt;p&gt;Summarization:
A machine learning system that categorizes messages by topic would be super helpful.
It could send a weekly digest listing the topics discussed, and clicking on a topic could bring me to a summary of the messages, with the option to drill down into the individual messages.&lt;/p&gt;
&lt;p&gt;Notification settings:
Right now I&amp;rsquo;ve simply turned off all notifications. If I could turn back on notifications for specific topics, that would be nice. E.g. &amp;ldquo;notify me if people start talking about programming tools again, but only if we haven&amp;rsquo;t talked about programming tools in at least a day since the last time you notified me.&amp;rdquo; This type of notification seems straightforward to build combining my Messager system with LLMs. Perhaps I&amp;rsquo;ll give it a go.&lt;/p&gt;
&lt;p&gt;Messenger UI: Even with notifications disabled, GGCs appear first in my list of Messenger chats because FB orders them by message recency, and GGCs always have a recent message. I&amp;rsquo;d prefer a way to keep them out of this default view, so I can see genuine conversations with individual people in their place.&lt;/p&gt;
&lt;p&gt;Browsing UI: The existing UI is unhelpful for trying to get an overview of the conversation. There&amp;rsquo;s a lot of whitespace, so only a few messages are visible at once. Replies for different topics are all mixed together.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If you&amp;rsquo;re working on anything like the toolings mentioned here, don&amp;rsquo;t hesitate to get in touch. There&amp;rsquo;s a Discussions section below, or feel free to ping me in a GGC!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Journal of Hypotheses</title>
      <link>https://davidbieber.com/snippets/2021-11-05-hypothesis-arxiv/</link>
      <pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-11-05-hypothesis-arxiv/</guid>
      
      <description>&lt;p&gt;This snippet is about an idea for a new venue for publishing research hypotheses before any experiments have been carried out.
We can think of the idea as a &amp;ldquo;Hypothesis Journal&amp;rdquo; or &amp;ldquo;Hypothesis Arxiv&amp;rdquo;. Similar to how arxiv.org hosts pre-prints for scholarly articles, the Hypothesis Arxiv would be a venue for researchers to submit their research hypotheses for the research community to see and evaluate. As I&amp;rsquo;m interested in having this venue serve as a place for the community to not only submit, but also discuss and evaluate the hypotheses, &amp;ldquo;Hypothesis OpenReview&amp;rdquo; might be an even more fitting title.&lt;/p&gt;
&lt;p&gt;To narrow the scope of discussion, let&amp;rsquo;s confine ourselves to thinking about machine learning research for the time being. My hope is that such a venue would allow the machine learning community to more thoughtfully select the set of hypotheses it focuses on from year to year.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll aim to cover (1) why this is a good idea, (2) what it might look like, and (3) I&amp;rsquo;ll try to preempt some concerns one might have. We&amp;rsquo;ll also take a quick look at (4) similar ideas that have been tried in the past, and what we can learn from them.&lt;/p&gt;
&lt;h3 id=&#34;the-machine-learning-hypotheses-journal&#34;&gt;The &amp;ldquo;Machine Learning Hypotheses&amp;rdquo; Journal&lt;/h3&gt;
&lt;p&gt;The &amp;ldquo;Hypothesis Journal&amp;rdquo; idea is for a new venue that accepts scholarly submissions from researchers in the form of hypotheses, rather than complete research papers.
A hypothesis is generally a simpler and shorter artifact than a research paper.
Whereas a research paper includes a hypothesis together with experiments, their results, analysis, and conclusions, a hypothesis paper stops short of performing the proposed experiments.
It still contains the background and motivation, hypothesis, and proposed experiments, but does not perform those experiments, gather any new data, or form any conclusions about the veracity of the hypothesis.&lt;/p&gt;
&lt;p&gt;Once hypothesis papers are submitted, they are assigned reviewers.
Reviewers evaluate hypothesis along various dimensions for their scientific merit, allowing those hypothesis that seem most promising (e.g. interesting, plausible and potentially surprising, ethical, and likely impactful) to rise to the top.&lt;/p&gt;
&lt;p&gt;In addition to the standard review period, an important piece of the idea is opening up discussion to the machine learning community at large. The difference here is that the official reviewers are selected because they are trusted experts in the same field as the hypothesis paper, whereas the public discussion is open to anyone.&lt;/p&gt;
&lt;p&gt;The hope is that through the expert reviews and the community discussions, strong hypotheses can be identified and rewarded, and all hypotheses can see sunlight that allows them both to improve and inspire.&lt;/p&gt;
&lt;p&gt;Since hypothesis papers take less effort and less time to produce, a researcher might produce and submit several hypothesis papers to the &amp;ldquo;Hypothesis Journal&amp;rdquo; in a single cycle. I am also hopeful that each hypothesis can be assigned a greater number of reviewers than conference papers typically are.
This would allow the variance of the reviewer pool to diminish a little, and for each hypothesis to receive valuable signal from a variety of reviewers as to its merits.&lt;/p&gt;
&lt;p&gt;To make the idea concrete, let&amp;rsquo;s call the journal &amp;ldquo;Machine Learning Hypotheses&amp;rdquo;. And let&amp;rsquo;s further say it has an annual submission deadline, and a short review cycle followed by a conference.&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The pace of machine learning research is accelerating. Certain types of machine learning research are becoming increasingly expensive to perform (e.g. training large scale language models), prohibitively so for some researchers. A result of this is that there are far more hypotheses to test than resources available to test them. So, choosing wisely what hypotheses we investigate is increasingly important.&lt;/p&gt;
&lt;p&gt;The current publication model incentives publication of complete results.
Researchers come together at conferences to discuss findings. It&amp;rsquo;s at this point that the most significant mixing of ideas occurs.
In a flow diagram, the scientific process looks like roughly like this, with the greatest discussion and mixing of ideas currently taking place at the point marked (♥), while I am proposing increasing our focus on the point marked (¿):&lt;/p&gt;
&lt;p&gt;[Observations (from scientific literature, experiments, etc.)]
&lt;br/&gt;➔ [Hypothesis (¿)]
&lt;br/&gt;➔ [Experiments]
&lt;br/&gt;➔ [Results]
&lt;br/&gt;Path 1: ➔ [Success] ➔ [Write up paper (♥)]
&lt;br/&gt;Path 2: ➔ [Failure (✝)] ➔ [Go to start. Revise Hypothesis]&lt;/p&gt;
&lt;p&gt;(♥): It is primarily at (♥), when a hypothesis has been confirmed, that the research community comes together en masse to discuss the findings. This takes place through publications, conferences, and journals.&lt;/p&gt;
&lt;p&gt;(✝): I have seen proposed on several occasions that there be more opportunities for publishing negative results, and I support these proposals. That amounts to greater discussion taking place at (✝), e.g. through the introduction of &amp;ldquo;Negative Result Journals&amp;rdquo;. This is not what I am proposing though; I am proposing increasing our community focus on (¿), when hypotheses are formed but not yet tested.&lt;/p&gt;
&lt;p&gt;(¿): My proposal is that we have a forum for publishing and evaluating hypotheses, even before experiments have been run and data has been collected, thereby increasing the amount of discussion of ideas that takes place at the spot marked (¿) in the ongoing cycle of the scientific method.&lt;/p&gt;
&lt;p&gt;The main benefit of increasing publication and discussion at the hypothesis stage of the scientific process is the potential to &lt;strong&gt;more selectively and thoughtfully shape the collective portfolio of hypotheses that the research community as a whole pursues&lt;/strong&gt;. It does this through a few mechanisms. I would summarize them as (1) &lt;strong&gt;quantity&lt;/strong&gt;: increasing the range and number of ideas generated, (2) &lt;strong&gt;quality&lt;/strong&gt; and (3) &lt;strong&gt;coordination&lt;/strong&gt;: improving the quality and diversity of research pursued through discussion, and (4) &lt;strong&gt;speed&lt;/strong&gt;: tightening the feedback loop between when an idea is generated and when downstream ideas based on it are generated.&lt;/p&gt;
&lt;h5 id=&#34;1-quantity&#34;&gt;(1) Quantity&lt;/h5&gt;
&lt;p&gt;It is much easier to submit a hypothesis than to submit a full research paper. Therefore a &amp;ldquo;hypothesis arxiv&amp;rdquo; or &amp;ldquo;hypothesis journal&amp;rdquo;, if adequately popular and prestigious, could substantially increase the number of ideas proposed and discussed widely compared with the number seen in publications today.&lt;/p&gt;
&lt;p&gt;Compared with writing up a hypothesis, actually performing experiments, collecting data, performing analysis, and drawing conclusions is a considerable amount of work. Some individual researchers might be able to submit a few times, up to perhaps a full order of magnitude, more high quality hypotheses than complete research projects. These might include ideas the researcher would never have time to pursue fully on their own, but which the community could benefit from greatly.&lt;/p&gt;
&lt;p&gt;By attaching credit and prestige to submitting hypothesis, we make it possible to split the role of researcher into two sub-roles. One sub-rule is the idea generator/&amp;ldquo;hypothesizer&amp;rdquo;, and the other is the &amp;ldquo;experimentalist&amp;rdquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; that evaluates the hypothesis. This would enable some researchers to specialize into one of these two categories (while many researchers could continue operating both roles concurrently). Today, a successful research career requires mastering both of these skill sets. A prestigious &amp;ldquo;hypothesis journal&amp;rdquo; could reduce this pressure, perhaps allowing for people to make careers out of just one skill set of the other, more than is possible today.&lt;/p&gt;
&lt;p&gt;Importantly, researchers would be able to submit hypotheses about experiments that they may be unable to carry out on their own. Take the case of training large language models as an example. This is a prohibitively expensive task for many research organizations. Even if you cannot train a large language model yourself, you can still learn about them and produce informed hypotheses about them.
So, the best ideas about large language models could come from someone not in a position to run the experiments themselves.
The &amp;ldquo;hypothesis arxiv&amp;rdquo; idea would allow these researchers to still play a vital role in the research process.
This idea could significantly expand the number of researchers who can participate in large language model research, even if many of them can only participate at the hypothesis stage&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h5 id=&#34;2-quality&#34;&gt;(2) Quality&lt;/h5&gt;
&lt;p&gt;By moving the discussion of hypotheses as a community earlier in the cycle of the scientific method, we can be more selective and focused in what hypotheses we dedicate significant resources to.
Ideas that are obviously true or obviously false can be identified by the review process; these ideas need not consume significant effort by the community. The best ideas will be identified as such, and lower quality ideas can improve through the feedback they receive during review and discussion. By getting more viewpoints involved in selecting and evaluating ideas, the best ideas (to a wider range of researchers) will rise to the top and garner more attention.&lt;/p&gt;
&lt;p&gt;When a hypothesis finally is confirmed or rejected, since it will already have been discussed by the community, we will also have a &lt;em&gt;more accurate idea of how surprising the result truly is&lt;/em&gt;. Without this discussion first, we can only use hindsight to estimate how surprising findings are. This is an important idea, which other research communities adopt partially through the concept of pre-registration, but which is somewhat lacking in machine learning research today.&lt;/p&gt;
&lt;h5 id=&#34;3-coordination&#34;&gt;(3) Coordination&lt;/h5&gt;
&lt;p&gt;Putting hypotheses out in public before running experiments also allows for improved coordination. In particular, it can &lt;em&gt;decrease overlap&lt;/em&gt; in ideas pursued. This can in turn allow more different research directions to be pursued.&lt;/p&gt;
&lt;p&gt;Additionally, improved coordination can allow for the formation of larger efforts around the most promising directions. For example, if a researcher proposes a large language model experiment that is promising but which they cannot evaluate themself, a large research org might choose to work directly with that researcher to collaboratively evaluate the hypothesis.&lt;/p&gt;
&lt;h5 id=&#34;4-speed&#34;&gt;(4) Speed&lt;/h5&gt;
&lt;p&gt;There are two senses in which the hypothesis journal could accelerate the research process. One is by allowing new ideas to be born sooner, and the second is by increasing competition.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Tighter iteration cycle&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Today an idea doesn&amp;rsquo;t spawn child ideas until it is published, which comes after the experiments have been run and the hypothesis evaluated.
In principle, these child ideas could be born much sooner if the hypothesis were shared sooner, even if the hypothesis has not been demonstrated true yet.
Of course the amount of downstream work that can be done is limited before a result has been confirmed, but it is limited in how limited it is.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Competition to evaluate ideas&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the &amp;ldquo;hypothesis journal&amp;rdquo; idea to work, there has to be some prestige or other motivations attached to submitting surprising, useful, high quality hypotheses that turn out to be true (or false in an interesting way). Even so, there will still be lots of motivation remaining for the groups that actually carry out the experiments and confirm or reject the hypotheses. Once a strong hypothesis paper has been submitted, the idea is public, and if it is a good idea there could be competition to evaluate the idea as quickly as possible.&lt;/p&gt;
&lt;h5 id=&#34;other-benefits&#34;&gt;Other benefits&lt;/h5&gt;
&lt;p&gt;Those four factors (quantity, quality, coordination, and speed) collectively suggest the hypothesis journal could accelerate the pace of machine learning research, and improve the quality of the collective portfolio of projects of the machine learning community. Other possible benefits of a hypothesis journal include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Increasing representation. (e.g. allowing groups to partake in the research process even when running experiments may be prohibitively expensive)&lt;/li&gt;
&lt;li&gt;Lowering the barrier to writing, thinking through thoroughly, sharing, and discussing ideas.&lt;/li&gt;
&lt;li&gt;Decoupling incentives, which could lead to more honest science. Specifically, if people are evaluating the hypotheses of others rather than their own hypotheses, then negative results suddenly become less unappealing. I could see this acting as a force that e.g. reduces p-hacking. Pre-registering ideas also comes with statistical benefits even if you&amp;rsquo;re the same person submitting the idea and doing the experimental science, because it reduces cherry-picking of results and similar effects.&lt;/li&gt;
&lt;li&gt;Decoupling the skill sets of hypothesizer from experimenter has value as well. It allows people to specialize, focusing their skill set and becoming better at their selected role.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;a-strong-hypothesis&#34;&gt;A Strong Hypothesis&lt;/h4&gt;
&lt;p&gt;What constitutes a complete hypothesis? What feedback can reviewers / the community provide about a hypothesis?&lt;/p&gt;
&lt;p&gt;When a researcher submits a &amp;ldquo;hypothesis paper&amp;rdquo;, it should be similar to a research paper that stops abruptly after proposing experiments, rather than continuing on to execute the experiments and analyze the results.&lt;/p&gt;
&lt;p&gt;It should include the hypothesis, stated precisely in a falsifiable way. It should motivate why it is a reasonable thing to believe, and why the authors expect it to be true, as well as why we do not already know that it is true. It should propose specific experiments, that in principle could be carried out (although perhaps at a budget exceeding that of the authors) to evaluate the hypothesis. It should include in the description of the experiments precisely what would be measured and why.&lt;/p&gt;
&lt;p&gt;My description here is perhaps a bit myopic, and the types of hypotheses researchers might submit might not conform to this set of criteria precisely, but I hope this conveys the sense of hypotheses that I have in mind. In a future snippet I can think at greater length about exactly what criteria make for a strong hypothesis.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a short incomplete checklist of ways a reviewer or the public might evaluate a submitted hypothesis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you believe it is true/false? Elaborate.&lt;/li&gt;
&lt;li&gt;Is it precisely stated?&lt;/li&gt;
&lt;li&gt;Is it feasible to evaluate? Does the proposed evaluation actually evaluate the hypothesis?&lt;/li&gt;
&lt;li&gt;Is it ethical?&lt;/li&gt;
&lt;li&gt;Is it worthwhile to pursue?
&lt;ul&gt;
&lt;li&gt;Cost&lt;/li&gt;
&lt;li&gt;Impact&lt;/li&gt;
&lt;li&gt;Downstream research unlocked&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Is it interesting / surprising?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The notion of &amp;ldquo;likely to be surprising&amp;rdquo; is interesting, and is an important one for a strong hypothesis.&lt;/p&gt;
&lt;h4 id=&#34;implementation-details&#34;&gt;Implementation details&lt;/h4&gt;
&lt;p&gt;As I mused in the introduction, &amp;ldquo;Hypothesis OpenReview&amp;rdquo; may be a more fitting name for the idea than &amp;ldquo;Hypothesis Arxiv&amp;rdquo;, and a journal or conference named &amp;ldquo;Machine Learning Hypotheses&amp;rdquo; is an appealing way to frame the venue for me. Perhaps OpenReview is already well suited for this idea. Instead of developing a new website, one possible implementation is to organize a new &lt;em&gt;conference&lt;/em&gt;, with submissions collected and discussed on OpenReview, where the gimmick of the conference is as discussed in this snippet: the authors need not perform the experiments they advocate for.&lt;/p&gt;
&lt;p&gt;How do we get people to (A) submit hypotheses, (B) review hypotheses, and (C) do both in a scholarly manner? The first step is to circulate this idea so we can discuss and improve upon it. If we can gather support (either for the idea as is, or a future modified version of it), from well respected researchers, their support could be instrumental in getting a &amp;ldquo;hypothesis journal&amp;rdquo; off the ground.&lt;/p&gt;
&lt;h4 id=&#34;similar-ideas&#34;&gt;Similar ideas&lt;/h4&gt;
&lt;p&gt;I&amp;rsquo;m aware of three ideas most similar to this one that have been tried in the past.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Grants&lt;/li&gt;
&lt;li&gt;Preregistration&lt;/li&gt;
&lt;li&gt;Medical Hypotheses&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;em&gt;grant application process&lt;/em&gt; is similar in many ways to the submission of a hypothesis paper. We can probably learn quite a bit from this process to inform how we implement the hypothesis journal idea. The key difference between our idea and a grant application is that there is not financing attached to submission of a hypothesis in our idea, and the person submitting a hypothesis need not be the same person who eventually evaluates it.&lt;/p&gt;
&lt;p&gt;Certain fields have a practice of pre-registration. In this practice, researchers publicly commit to a research plan before beginning the research. Wikipedia suggests preregistration can reduce p-hacking, publication bias, data dredging, inappropriate post hoc analysis, and HARKing. A hypothesis journal can have many similar benefits as preregistration. A key difference between submitting a hypothesis and pre-registering a research plan is that submitting a hypothesis does not commit you to performing the research, and in fact it may be appropriate for someone else (or no one at all) to perform the hypothesis evaluation, rather than its author.&lt;/p&gt;
&lt;p&gt;Finally there is one journal &lt;em&gt;Medical Hypotheses&lt;/em&gt; that has a similar motivation (albeit less similar than the journal&amp;rsquo;s name suggests) to our hypothesis journal idea. &lt;em&gt;Medical Hypotheses&lt;/em&gt; was designed to allow discussion of unconventional ideas, which is a different goal than we are pursuing. Like with our idea, the experiments to validate the ideas need not be run yet. This journal published controversial papers on AIDS denialism, and for most of its existence largely forwent peer review.
We can probably learn quite a bit from the history of &lt;em&gt;Medical Hypotheses&lt;/em&gt;. One lesson is that peer review and the publication of hypotheses are orthogonal; I suggest we keep peer review, while still encouraging the submission of hypotheses before they have been evaluated.&lt;/p&gt;
&lt;h4 id=&#34;concerns&#34;&gt;Concerns&lt;/h4&gt;
&lt;p&gt;In my brainstorming, I anticipate a handful of potential concerns about the idea.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flag-planting&lt;/li&gt;
&lt;li&gt;Scooping&lt;/li&gt;
&lt;li&gt;Prestige / Motivation for participation&lt;/li&gt;
&lt;li&gt;Collapsing of the idea space (need divergence and convergence; don&amp;rsquo;t want to be left with an overabundance of convergence and insufficient divergence)&lt;/li&gt;
&lt;li&gt;Ideas are a dime a dozen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I am not so concerned about &lt;em&gt;flag-planting&lt;/em&gt;. The primary purpose of the idea is to share hypotheses and inspire others to evaluate or build off of them. Flag-planting is a problem if it involves people claiming overly broad ideas prematurely so that they get future credit once the idea is more carefully analyzed and the specific instantiation of the idea that is most important comes to light.
Credit is a finicky thing, but from my perspective it is rare in modern science that full credit is attributed to a single individual or single group; the story is always more complicated. If someone proposes an overly broad hypothesis, effectively flag-planting, then perhaps they will in turn receive a portion of the credit for the specific important idea that develops from it. I do not find this objectionable. I would be curious to discuss this with you if you find flag-planting a larger concern than I am making it out to be with this idea.&lt;/p&gt;
&lt;p&gt;A fear of &lt;em&gt;scooping&lt;/em&gt; could certainly prevent people from submitting ideas to the &amp;ldquo;Machine Learning Hypothesis&amp;rdquo; journal. However, I expect that most researchers have a long line of hypotheses that they will never have the time to evaluate. So, my hope is that researchers who fear scooping could choose to keep the hypotheses they are actively working on a secret, while still opening up and sharing the many others that they possess.
For many researchers, even publishing the hypotheses that they are actively working on will be worthwhile. This allows them to get (partial) credit for the idea earlier (in effect preventing a form of scooping) than if they had to finish the research paper before submitting the idea, and it allows for earlier feedback on their research which usually leads to higher quality research overall.&lt;/p&gt;
&lt;p&gt;Next comes the issue of &lt;em&gt;motivating&lt;/em&gt; people to submit to the conference. I think that we could actually make this conference the most interesting and most fun of the machine learning conferences. Discussing half-baked ideas with researchers is always enjoyable, and I am optimistic a significant fraction of the ML research community could enjoy both submitting to this journal, and reviewing for it, because doing both is a great exercise in research creativity. Prestige and funding are other motivators for researchers; I do not think a hypothesis journal could ever be as prestigious as typical top ML conferences, but I do think that with the backing of strong players and institutions in the research community, sufficient prestige could be attached to strong submissions to the conference/journal.&lt;/p&gt;
&lt;p&gt;The next concern is about &lt;em&gt;collapsing of the idea space&lt;/em&gt;. In order for a group to come up with the best possible ideas, there needs to be space for the group&amp;rsquo;s thinking to diverge, as well as space for discussion leading to some amount of convergence on good ideas. The hypothesis journal idea increases the amount of convergence. Some might argue we actually need more divergence in our thinking. I would be interested to discuss this with you if you have this concern.&lt;/p&gt;
&lt;p&gt;Finally, a possible concern is that hypotheses without experiments are not on their own valuable, e.g. that &amp;ldquo;ideas are a dime a dozen&amp;rdquo;. To address this concern, I think we merely need to raise the bar about what constitutes a good hypothesis. We&amp;rsquo;re not looking for just any old idea, but rather ones that will advance the science of machine learning, backed up by the evidence the machine learning community has collected, and likely to be impactful on future science. Coming up with a good hypothesis is not easy, but a popular hypothesis journal could be an effective tool for improving our collective ability to do so.&lt;/p&gt;
&lt;p&gt;If you share these concerns of have additional ones not discussed here, please get in touch. I&amp;rsquo;d love to discuss.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I describe the &amp;ldquo;hypothesis arxiv&amp;rdquo; idea using the language of empirical science, but in principle the idea could be applied to entirely theoretical fields as well. In that setting, the idea would be to make space for unproven conjectures to be shared more freely. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;And if a hypothesis submitted to the hypothesis arxiv by one group is being evaluated by another group, then it would make sense to me to include the first group closely in the evaluation process. In this way, the &amp;ldquo;hypothesis arxiv&amp;rdquo; may open up LLM research even more than described in the main text. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning and Automation Ideas for Note-taking</title>
      <link>https://davidbieber.com/snippets/2021-10-28-ml-and-automation-for-note-taking/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-10-28-ml-and-automation-for-note-taking/</guid>
      
      <description>&lt;p&gt;Let&amp;rsquo;s consider machine learning first, and then automation. For each we brainstorm the potential value for note-taking.&lt;/p&gt;
&lt;h3 id=&#34;the-value-of-markmachine-learningmark-for-note-taking&#34;&gt;The Value of &lt;mark&gt;Machine Learning&lt;/mark&gt; for Note-Taking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Automatic &lt;strong&gt;transcription&lt;/strong&gt; of audio notes
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speaker identification&lt;/strong&gt; during meetings or conversations&lt;/li&gt;
&lt;li&gt;Resurfacing of action items, surfacing relevant information, and performing actions (e.g running a google search or sending a message) in response to audio cues during meetings or conversations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Automatic &lt;strong&gt;categorization&lt;/strong&gt; of notes&lt;/li&gt;
&lt;li&gt;Automatic &lt;strong&gt;categorization of spaced repetition&lt;/strong&gt; flash cards&lt;/li&gt;
&lt;li&gt;Automatic &lt;strong&gt;context-filling&lt;/strong&gt; of spaced repetition flash cards
&lt;ul&gt;
&lt;li&gt;e.g. highlight a phrase to create a flash card; the necessary context is pulled in from surrounding paragraphs to make a coherent standalone flash card.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Automatic &lt;strong&gt;connections&lt;/strong&gt; and graph construction:
&lt;ul&gt;
&lt;li&gt;Textual elements:
&lt;ul&gt;
&lt;li&gt;Entity extraction from notes (&amp;ldquo;implicit nodes&amp;rdquo;, e.g. &amp;ldquo;Bank of America&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;Attribute extraction from notes (e.g. &amp;ldquo;incredible presentation by Pierre-Antoine&amp;rdquo; gets tagged as incredible or wow (the canonical form).)&lt;/li&gt;
&lt;li&gt;Relationship extraction (e.g. &amp;ldquo;Diamond Age by Neal Stephenson&amp;rdquo; -&amp;gt; creates an &amp;ldquo;author of&amp;rdquo; relationship)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Background knowledge elements:
&lt;ul&gt;
&lt;li&gt;e.g. knowing that Diamond Age is a book&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Writing prompt generator / &lt;strong&gt;Writing assistant&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overlap detection&lt;/strong&gt; between people&amp;rsquo;s graphs
&lt;ul&gt;
&lt;li&gt;The Go Note Go &amp;ldquo;clink&amp;rdquo; scenario&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Autocomplete&lt;/strong&gt; (e.g. gmail tab completion)&lt;/li&gt;
&lt;li&gt;Improved &lt;strong&gt;spaced repetition algorithms&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Mistake detection / &lt;strong&gt;fact checking&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Summarization&lt;/strong&gt; of notes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Highlight extraction&lt;/strong&gt; from notes&lt;/li&gt;
&lt;li&gt;Suggesting &lt;strong&gt;new ideas&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Suggesting &lt;strong&gt;new online resources&lt;/strong&gt; to read / explore
&lt;ul&gt;
&lt;li&gt;Incl. articles, papers, YouTube videos, Suggesting &lt;strong&gt;people to reach out to&lt;/strong&gt; (e.g. because of their blog or Twitter or other things they&amp;rsquo;re written)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semantic search&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conversational chat bots&lt;/strong&gt; for exploring the knowledge in a notebase&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-value-of-markautomationmark-for-note-taking&#34;&gt;The Value of &lt;mark&gt;Automation&lt;/mark&gt; for Note-Taking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Share notes&lt;/strong&gt; with people via their preferred messaging platform&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Snooze notes&lt;/strong&gt; and ideas so you review them later&lt;/li&gt;
&lt;li&gt;Set pomodoros, alarms, and reminders to &lt;strong&gt;stay focused&lt;/strong&gt; and &lt;strong&gt;stay on task&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Note-taking &lt;strong&gt;via highlights&lt;/strong&gt; (e.g. with Browserflow)&lt;/li&gt;
&lt;li&gt;Note-taking &lt;strong&gt;via screenshots&lt;/strong&gt; (e.g. from YouTube)&lt;/li&gt;
&lt;li&gt;Automatic creation of &lt;strong&gt;spaced repetition from highlights&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bulk refactors&lt;/strong&gt; of note-taking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pulling in context&lt;/strong&gt; from other sources (e.g. Warmly pulls LinkedIn info into Calendar, notes could pull info from wikipedia)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sync notes&lt;/strong&gt; between other sources and your notebase
&lt;ul&gt;
&lt;li&gt;Other people&amp;rsquo;s notes&lt;/li&gt;
&lt;li&gt;Your email client, calendar, TODO app, spaced repetition app, GitHub issues, other messaging services, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publishing&lt;/strong&gt; of notes to a webpage (either publicly or privately), or to social media&lt;/li&gt;
&lt;li&gt;New &lt;strong&gt;data entry&lt;/strong&gt; mechanisms: &lt;strong&gt;Go Note Go&lt;/strong&gt;, terminal, email, text, browsing history, location history, terminal history, IoT devices, etc&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spaced repetition&lt;/strong&gt; for studying&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inactivity reminders&lt;/strong&gt; (e.g. &amp;ldquo;it&amp;rsquo;s been three months since you&amp;rsquo;ve done a music activity&amp;rdquo;, or &amp;ldquo;it&amp;rsquo;s been over 6 months since you&amp;rsquo;ve spoken with David Dohan&amp;rdquo;, or &amp;ldquo;it&amp;rsquo;s been over a year since you&amp;rsquo;ve written about microbiology.&amp;quot;)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GPT-3 for Spaced Repetition</title>
      <link>https://davidbieber.com/snippets/2021-09-08-gpt-3-for-spaced-repetition/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-09-08-gpt-3-for-spaced-repetition/</guid>
      
      <description>&lt;p&gt;Lately I&amp;rsquo;ve been using Browserflow to create spaced repetition flashcards quickly while browsing the internet.&lt;/p&gt;
&lt;p&gt;To create a card, I highlight a bit of text and press &amp;ldquo;c&amp;rdquo;. This copies the sentence containing the highlight into the card, with the part I highlighted masked out.
It&amp;rsquo;s a cloze task, as in a masked language model.
If instead I push &amp;ldquo;b&amp;rdquo;, it copies the full paragraph containing the highlight, again with the highlight masked out.&lt;/p&gt;
&lt;p&gt;Usually this is a great way to quickly create my flashcards. However, sometimes I get sentences where the subject is unclear without context, e.g. &amp;ldquo;this&amp;rdquo; in &amp;ldquo;Usually this is a great way to quickly create the flashcards.&amp;rdquo; For these, it would be great to automatically rewrite the sentence with the subject pulled in from the surrounding context.&lt;/p&gt;
&lt;p&gt;For example, &amp;ldquo;Usually this is a great way to quickly create the flashcards&amp;rdquo; could be rewritten as &amp;ldquo;Usually &lt;mark&gt;using &amp;ldquo;b&amp;rdquo; and &amp;ldquo;c&amp;rdquo; with Browserflow&lt;/mark&gt; is a great way to quickly create the flashcards.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;As a second example, consider this 
&lt;a href=&#34;http://hyperphysics.phy-astr.gsu.edu/hbase/Solar/meteor.html#c3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HyperPhysics page on the Murchison Meteorite&lt;/a&gt;. Speaking about the Murchison Meteorite, the page says &amp;ldquo;Seventy four amino acids were found in it, compared to the 20 which are characteristic of life.&amp;rdquo; I&amp;rsquo;d love to make a card saying &amp;ldquo;Seventy four &lt;mark&gt;amino acids&lt;/mark&gt; were found in the Murchison Meteorite, compared to the 20 which are characteristic of life.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;However, if I highlight &lt;mark&gt;amino acids&lt;/mark&gt; and press &amp;ldquo;c&amp;rdquo;, the generated card instead reads: &amp;ldquo;Seventy four &lt;mark&gt;amino acids&lt;/mark&gt; were found in &lt;em&gt;it&lt;/em&gt;, compared to the 20 which are characteristic of life.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;So, the challenge I present you with is: write a program that takes a sentence, and it&amp;rsquo;s surrounding context, and rewrites the sentence so that it is clear even out of context.&lt;/p&gt;
&lt;p&gt;Having played with GPT-3 and Codex recently, this seems well within the scope of large pretrained language models. I expect that with some careful prompting, a large language model could excel at this task without even any fine-tuning, just taking advantage of few shot learning. Is this the best or simplest approach? Likely it isn&amp;rsquo;t, but it may be the approach that minimizes development time and is easiest to repurpose for other similar (or dissimilar) projects in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Codex as a Programming Language</title>
      <link>https://davidbieber.com/snippets/2021-08-16-codex-pl/</link>
      <pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-08-16-codex-pl/</guid>
      
      <description>&lt;p&gt;In the 
&lt;a href=&#34;https://www.youtube.com/watch?v=SGUCcjHTmGY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;impressive demo of OpenAI&amp;rsquo;s &lt;em&gt;Codex&lt;/em&gt; programming system&lt;/a&gt;,
the presenters suggest that Codex enables natural language to be used as a new kind of programming language.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how that might look, followed by one problem with the suggestion.&lt;/p&gt;
&lt;p&gt;A natural-language programmer writes natural language prompts, and Codex transforms them into source code.
The Codex model is made deterministic, such that rerunning the model on the prompts always produces the same source code.
The user can make further modifications to the source code that the model produces.
The result is two parallel streams of text: (1) the natural language, and (2) the final source code.
These parallel streams can be saved in version control, shared, modified, remixed, and run.&lt;/p&gt;
&lt;p&gt;A non-programmer might still be able to make modifications, simply by modifying the natural language text, without making modifications to the source code at all.
This could be a way to learn a bit about programming for the curious non-programmer too, as they can observe the changes to the source code
that result from changing the natural language text.&lt;/p&gt;
&lt;p&gt;It is tempting to think that only the diff between the source code produced by the model and the final source code need be stored.
However, this doesn&amp;rsquo;t hold up. The prompt when generating source code includes all earlier source code in the file. So if a programmer wants to modify
a natural language prompt or piece of code earlier in a file, it could invalidate all the generated code later in the file. Rerunning the model on the new state of the program wouldn&amp;rsquo;t reproduce the source code verbatim, even with the model made deterministic.&lt;/p&gt;
&lt;p&gt;The editor showing the program might be modified to reflect this. I envision it showing two columns, the left column with natural language text and the right column, aligned with the left, showing the source code. A block of text and code shows with a green background if the model, when run on the text, produces exactly the source code on the right. It shows with yellow if the source code produced by the model differs slightly from the actual code in the block (and the diff is highlighted). And it shows in red if no simple diff distinguishes the model&amp;rsquo;s output from the actual code in the block.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Derivative of Softmax and the Softmax Cross Entropy Loss</title>
      <link>https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/</link>
      <pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/</guid>
      
      <description>&lt;p&gt;Write $y_i = \text{softmax}(\textbf{x})_i = \frac{e^{x_i}}{\sum e^{x_d}}$.&lt;/p&gt;
&lt;p&gt;That is, $\textbf{y}$ is the softmax of $\textbf{x}$. Softmax computes a normalized exponential of its input vector.&lt;/p&gt;
&lt;p&gt;Next write $L = -\sum t_i \ln(y_i)$. This is the softmax cross entropy loss. $t_i$ is a 0/1 target representing whether the correct class is class $i$. We will compute the derivative of $L$ with respect to the inputs to the softmax function $\textbf{x}$.&lt;/p&gt;
&lt;p&gt;We have $\frac{dL}{dx_j} = -\sum t_i \frac{1}{y_i} \frac{dy_i}{d{x_j}}$ from the chain rule.&lt;/p&gt;
&lt;p&gt;We compute $\frac{dy_i}{dx_j}$ using the quotient rule.&lt;/p&gt;
&lt;p&gt;If $i = j$, this gives:&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \frac{\sum e^{x_d} \cdot e^{x_i} - e^{x_i} \cdot e^{x_i}}{(\sum e^{x_d})^2}$&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \frac{e^{x_i}}{\sum e^{x_d}} \cdot \left(\frac{\sum e^{x_d} - e^{x_i}}{\sum e^{x_d}}\right)$&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = y_i \cdot (1 - y_i)$&lt;/p&gt;
&lt;p&gt;If $i \ne j$, this gives:&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \frac{\sum e^{x_d} \cdot 0 - e^{x_i} \cdot e^{x_j}}{(\sum e^{x_d})^2}$&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = -\frac{e^{x_i}}{\sum e^{x_d}} \cdot \frac{e^{x_j}}{\sum e^{x_d}} $&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = -y_i y_j$&lt;/p&gt;
&lt;p&gt;Together these equations give us the derivative of the softmax function:&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \begin{cases} y_i \cdot (1 - y_i) &amp;amp; i=j \\\ -y_i y_j &amp;amp; i \ne j \end{cases}$&lt;/p&gt;
&lt;p&gt;Using this result, we can finish computing the derivative of $L$. This gives:&lt;/p&gt;
&lt;p&gt;$\frac{dL}{dx_j} = -\sum t_i \frac{1}{y_i} \frac{dy_i}{d{x_j}} = \sum\limits_i \begin{cases} t_i (y_i - 1) &amp;amp; i=j \\\ t_i y_j &amp;amp; i \ne j \end{cases}$&lt;/p&gt;
&lt;p&gt;Since exactly one of the $t_i$s is 1 and the rest are zeros this further simplifies to:&lt;/p&gt;
&lt;p&gt;$\frac{dL}{dx_j} = y_j - t_j$&lt;/p&gt;
&lt;p&gt;We have computed the derivative of the softmax cross-entropy loss $L$ with respect to the inputs to the softmax function.&lt;/p&gt;
&lt;p&gt;This page is an experiment in publishing directly from Roam Research. &lt;del&gt;It is incomplete, and the formatting is probably all wonky. Bear with me while I get this sorted.&lt;/del&gt; Update (December 13th, 2020): The formatting looks good now!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing with GPT-3</title>
      <link>https://davidbieber.com/snippets/2020-07-22-writing-with-gpt3/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-07-22-writing-with-gpt3/</guid>
      
      <description>&lt;p&gt;GPT-3 is a language model developed at Open AI, and I&amp;rsquo;ve been having a lot of fun playing with it. In fact, I&amp;rsquo;ve written this post in collaboration with GPT-3. See if you can spot which parts are written by me and which parts come from GPT-3.&lt;/p&gt;
&lt;h2 id=&#34;part-1-brainstorming&#34;&gt;Part 1: Brainstorming&lt;/h2&gt;
&lt;p&gt;The first thing I did when I got access to GPT-3 is use it to brainstorm uses for GPT-3. I gave it these 5 ideas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explain mathematics more simply (why is X true?)&lt;/li&gt;
&lt;li&gt;Generate programs from input/output examples&lt;/li&gt;
&lt;li&gt;Generate questions about a paper I read&lt;/li&gt;
&lt;li&gt;Create a list of machine translation tips and tricks&lt;/li&gt;
&lt;li&gt;Create an adventure game&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And it came back with 15 more:&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Create a list of Tor commands to run to defend against an attack&lt;/li&gt;
&lt;li&gt;Create a cheat sheet for screenwriters&lt;/li&gt;
&lt;li&gt;Generate advice for a scene&lt;/li&gt;
&lt;li&gt;Generate XKCD comics&lt;/li&gt;
&lt;li&gt;Create a crib sheet for an arbitrary Python API&lt;/li&gt;
&lt;li&gt;Make a list of movies to watch&lt;/li&gt;
&lt;li&gt;Make a list of scholarly algorithms/heuristics worth reading&lt;/li&gt;
&lt;li&gt;Make a list of generic questions about Pedagogy&lt;/li&gt;
&lt;li&gt;Make a list of cognitive exercises&lt;/li&gt;
&lt;li&gt;Show patterns generated by awesome people&lt;/li&gt;
&lt;li&gt;Generate small cooking tasks&lt;/li&gt;
&lt;li&gt;Generate characters for a project&lt;/li&gt;
&lt;li&gt;Generate configuration options&lt;/li&gt;
&lt;li&gt;Generate flashcards&lt;/li&gt;
&lt;li&gt;Organize my computer/files&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And then 15 more:&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Explain things to my parents via messenger&lt;/li&gt;
&lt;li&gt;Explain things to my children via messenger&lt;/li&gt;
&lt;li&gt;Explain mathematical algorithms&lt;/li&gt;
&lt;li&gt;Automated interface design&lt;/li&gt;
&lt;li&gt;Mechanize a Nagihara Kan Airlines website&lt;/li&gt;
&lt;li&gt;Clickbait generator&lt;/li&gt;
&lt;li&gt;Music description (Herbie Hancock is playing &amp;ldquo;Dolphin Dance&amp;rdquo; live in London on the 18th March 1968)&lt;/li&gt;
&lt;li&gt;Tell me not to waste my time looking at x&lt;/li&gt;
&lt;li&gt;Interesting facts about Wikipedia&lt;/li&gt;
&lt;li&gt;Explain something to someone while my attention is elsewhere&lt;/li&gt;
&lt;li&gt;Detect adverts in x&lt;/li&gt;
&lt;li&gt;Image caption/caption game&lt;/li&gt;
&lt;li&gt;Speculate about the true utility function of x&lt;/li&gt;
&lt;li&gt;Auto-generate tests for a program&lt;/li&gt;
&lt;li&gt;Explain why my code is currently failing tests.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And then 15 more:&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Generate plots or character/story summaries&lt;/li&gt;
&lt;li&gt;Generate legal examples&lt;/li&gt;
&lt;li&gt;Generate words for crosswords&lt;/li&gt;
&lt;li&gt;Check my grammar&lt;/li&gt;
&lt;li&gt;Supply filler text for themed writing (horror, sci-fi etc.)&lt;/li&gt;
&lt;li&gt;Explain why an AI can&amp;rsquo;t read (can it comprehend metaphors?)&lt;/li&gt;
&lt;li&gt;Provide entertaining answers to obscure questions&lt;/li&gt;
&lt;li&gt;Get writing ideas and generate word combinations&lt;/li&gt;
&lt;li&gt;Create background for an NPC I need for a D&amp;amp;D game&lt;/li&gt;
&lt;li&gt;Help a sick writer generate plots&lt;/li&gt;
&lt;li&gt;Input pre-generated (from books etc.) phrases and use them for literary analysis&lt;/li&gt;
&lt;li&gt;Help explain a mathematical proof&lt;/li&gt;
&lt;li&gt;Develop a game/story/character I need&lt;/li&gt;
&lt;li&gt;Make short stories I can &amp;ldquo;overlay&amp;rdquo; on a fictional Universe I&amp;rsquo;ve created (ala Ready Player One)&lt;/li&gt;
&lt;li&gt;Make a short film/audio drama&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What a lovely list of lists of ideas! I wonder how many of these ideas are novel.&lt;/p&gt;
&lt;p&gt;Some of these ideas are outside the scope of what GPT-3 can do. For example, generating XKCD comics seems too difficult. Others are duplicates, such as &amp;ldquo;Explain mathematical algorithms&amp;rdquo; (duplicate of &amp;ldquo;Explain mathematics more simply&amp;rdquo;). And still others don&amp;rsquo;t make sense &amp;ndash; &amp;ldquo;Mechanize a Nagihara Kan Airlines website&amp;rdquo;? Overall though, solid set of ideas.&lt;/p&gt;
&lt;h2 id=&#34;part-2-mathematics&#34;&gt;Part 2: Mathematics&lt;/h2&gt;
&lt;p&gt;Once I&amp;rsquo;d gotten GPT-3 to do some brainstorming, I realized I&amp;rsquo;d need to test its capabilities before trying its suggestions out in earnest.&lt;/p&gt;
&lt;p&gt;I taught it how to evaluate simple expressions and take derivatives.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the first prompt I used:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;Question: f(x) = x*x. Compute f(3).
f(3) = 3*3
f(3) = 9
Answer: 9

Question: h(x) = x*x - 2x. Compute h(5).
h(5) = 5*5 - 2*5
h(5) = 25 - 10
h(5) = 15
Answer: 15

Question: y(x) = x**3 - 2x. Compute y(3).
y(3) = 3**3 - 2*3
y(3) = 3*3*3 - 6
y(3) = 27 - 6
y(3) = 21
Answer: 21

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I could then ask it any of the following, and it would correctly show its work.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;Question: f(x) = tan(x) + 1. Compute f(0).
Question: f(x) = sqrt(x) + 1. Compute f(16).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I tried to give it&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;Question: f(x) = x/2 + 2. Compute f(f(16)).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it treated it as just &lt;code&gt;Compute f(16)&lt;/code&gt; rather than &lt;code&gt;f(f(16))&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Interestingly this works:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;Question: f(x) = x*x. Compute f(3).
f(3) = 3*3
f(3) = 9
Answer: 9

Question: h(x) = x*x - 2x. Compute h(5).
h(5) = 5*5 - 2*5
h(5) = 25 - 10
h(5) = 15
Answer: 15

Question: f(x) = x/2 + 2. Compute f(f(16)).
f(f(16)) = f(16/2 + 2)
f(f(16)) = f(8 + 2)
f(f(16)) = f(10)
f(f(16)) = 10/2 + 2
f(f(16)) = 5 + 2
f(f(16)) = 7
Answer: 7

Question: y(x) = x**3 - 2x. Compute y(3).
y(3) = 3**3 - 2*3
y(3) = 3*3*3 - 6
y(3) = 27 - 6
y(3) = 21
Answer: 21

Question: f(x) = x*3 - 1. Compute f(f(5)).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas this does not:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;Question: f(x) = x*x. Compute f(3).
f(3) = 3*3
f(3) = 9
Answer: 9

Question: h(x) = x*x - 2x. Compute h(5).
h(5) = 5*5 - 2*5
h(5) = 25 - 10
h(5) = 15
Answer: 15

Question: f(x) = x*3 - 1. Compute f(f(5)).
f(f(5)) = f(5*3 - 1)
f(f(5)) = f(15 - 1)
f(f(5)) = f(14)
f(f(5)) = 14*3 - 1
f(f(5)) = 42 - 1
f(f(5)) = 41
Answer: 41

Question: y(x) = x**3 - 2x. Compute y(3).
y(3) = 3**3 - 2*3
y(3) = 3*3*3 - 6
y(3) = 27 - 6
y(3) = 21
Answer: 21

Question: f(x) = x/2 + 2. Compute f(f(16)).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After getting it to evaluate these simple expressions correctly, I also tried having it take derivatives. With an example as priming, and again having it show its work, this worked well. I haven&amp;rsquo;t performed a rigorous evaluation of its mathematical capabilities. For now, I&amp;rsquo;ve just begun to develop an intuition for how it works.&lt;/p&gt;
&lt;h2 id=&#34;part-3-playing-around&#34;&gt;Part 3: Playing Around&lt;/h2&gt;
&lt;p&gt;After doing math together, I tried many more things with GPT-3. I shared feelings with it, and it expanded on them (correctly too!). I used it to generate lists for brainstorming all manner of thing. The language model wrote love letters from inanimate objects. I tried to teach it to play Codenames, but with only very limited success. And I had it autocomplete previous snippets of mine, and had some fun cowriting with it. While I won&amp;rsquo;t be sharing all of this with you, I do have one thing to share.&lt;/p&gt;
&lt;p&gt;This is a love letter written by a frying pan:&lt;/p&gt;
&lt;p&gt;I may not seem like much to you right now. I&amp;rsquo;m coated in grease and I&amp;rsquo;m starting to rust, but really, I have big plans. I&amp;rsquo;m going to Italian restaurants. I&amp;rsquo;ll hang out with the finest (and healthiest) ingredients you can imagine. It was good knowing you, but you were never mine to keep. I&amp;rsquo;m going to cook the kind of meal you&amp;rsquo;ll dream about for the rest of your life. Good luck, sweetheart. You&amp;rsquo;ll need it.&lt;/p&gt;
&lt;h2 id=&#34;overall-principles&#34;&gt;Overall Principles&lt;/h2&gt;
&lt;p&gt;Getting an intuition for how GPT-3 behaves has been interesting, and I&amp;rsquo;ll try now to distill that intuition down into a few principles.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GPT-3 is continuing your writing, not having a dialog with you.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;ve seen on Twitter lots of people asking GPT-3 questions, and sharing its response. Know that GPT-3 continues whatever prompt you give it as if it had written the prompt itself, not as if it is responding to the prompt. This is why you often see prompts like &amp;ldquo;The following is a conversation between X and Y:&amp;rdquo;.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;GPT-3 effectively puts your writing in a made up context.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you give GPT-3 a small prompt, such a single sentence, then there are many contexts in which that prompt could be interpreted. GPT-3 is trained to marginalize over the contexts in which the text could appear. Once you start decoding GPT-3&amp;rsquo;s response though, one implicit context has been selected. It might not be the context that you intended when writing your prompt.&lt;/p&gt;
&lt;p&gt;This can happen even if you try to set the context yourself explicitly. For example you might be trying to generate a conversation or a list, and GPT-3 might decide that this conversation or list is taking place inside of a novel.&lt;/p&gt;
&lt;p&gt;You can mitigate this context switch by using a longer prompt, or by doing more of the writing yourself.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;GPT-3 likes lists and repeated structures.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is the main principle behind using GPT-3 for few shot learning. Prompt GPT-3 with a few examples of performing a task, and it will usually make an effort to perform that task yet again.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;GPT-3 cannot perform linguistic tricks like unscrambling words&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It doesn&amp;rsquo;t seem capable of generating new puns (I haven&amp;rsquo;t tried this yet).
And it doesn&amp;rsquo;t seem capable of unscrambling words (I did try this, and I couldn&amp;rsquo;t even get it to unscramble 4-letter words).&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;GPT-3 benefits from &amp;ldquo;showing its work&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my math experiments, having GPT-3 show its work was critical to getting it solving the math questions correctly. It seems that doing a single step of expression evaluation at a time is an easier task than doing it all at once. Providing examples with the steps gave GPT-3 a clearer explanation of the task than just providing input/output examples. GPT-3 also gets additional computational steps in which to solve the problem when it takes the time to show its work.&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;GPT-3 has a great breadth of knowledge and writing styles available to it&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It can express love, write flowery prose, give advice, write technical documentation, latex, program, math, take the perspective of a toaster, and more. It&amp;rsquo;s a great tool for brainstorming both explicitly (by having it generate ideas) and through exploration (by using it to help you generate ideas yourself).&lt;/p&gt;
&lt;p&gt;Though it has all the facts from the internet at its fingertips (of all the ways to anthropomorphize GPT-3, why choose fingertips?), it is just as capable of generating falsehoods as truths. Do not mistake the text it generates as true or logically correct, even if in many circumstances it does generate correct information. When it transitions from generating truth to generating nonsense it does not give a warning that it has done so (and any truth it does generate is in a sense at least partially accidental).&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;Sometimes GPT-3 makes no sense&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are things that don&amp;rsquo;t make sense that the language model assigns reasonable probabilities to. And there are things that don&amp;rsquo;t make sense, are low probability, but come up anyway either due to chance or because the model doesn&amp;rsquo;t have anything better to say. It doesn&amp;rsquo;t stop generating text just because its out of its depth. Instead, it just plods on forward, saying something as reasonable as it&amp;rsquo;s capable of, regardless of the situation it finds itself in.&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;GPT-3 mimics your style&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you prompt GPT-3 with typos it will continue with typos. If you prompt GPT-3 with florid prose, it will write florid prose. If you prompt GPT-3 with anaphora laden rhetoric, or any other style, it will continue in that style.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll let GPT-3 wrap things up itself:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;And that brings us to the end, some thoughts on whether using GPT-3 was more or less crazy than getting a Ph.D.. I think that using GPT-3 to generate ideas was a higher form of Enlightenment than any Ph.D. I&amp;rsquo;d do that again in a heartbeat, no shortage of ideas rolling around in GPT-3&amp;rsquo;s natural language processing head.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Uh huh. Whatever, GPT-3. Whatever.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>