<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>brains on David Bieber</title>
    <link>https://davidbieber.com/tags/brains/</link>
    <description>Recent content in brains on David Bieber</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://davidbieber.com/tags/brains/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>We Could Use Augmented Reality to Simulate Prosopagnosia</title>
      <link>https://davidbieber.com/snippets/2021-12-12-prosopagnosia-augmented-reality/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-12-12-prosopagnosia-augmented-reality/</guid>
      
      <description>&lt;p&gt;I was watching 
&lt;a href=&#34;https://www.youtube.com/watch?v=vFZY--lgmHs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture 4 of Nancy Kanwisher&amp;rsquo;s class &amp;ldquo;The Human Brain&amp;rdquo;&lt;/a&gt;,
and it occurred to me that we could use augmented reality to simulate prosopagnosia.
Prosopagnosia, for those unfamiliar with the term, is &amp;ldquo;face blindness&amp;rdquo;.
It affects about 2% of the population with varying degrees of severity,
and those with prosopagnosia cannot recognize faces.&lt;/p&gt;
&lt;p&gt;In the in-person version of the course,
the students observe demos to show off the importance of motion in vision, and color in vision.
Those demos aren&amp;rsquo;t available online; they only work in person.
The color vision demo puts students in a room without colored light, thereby only permitting gray-scale vision.
The importance of color then becomes immediately apparent, no pun intended; color is useful for distinguishing safe food from spoiled, and healthy faces from sickly ones.
This exercise helps motivate Marr&amp;rsquo;s theory of computation: that you can reason about brain function by thinking about what the brain must compute, and why.&lt;/p&gt;
&lt;p&gt;In a similar spirit to these demos,
there could also be a demo for the students showing what it&amp;rsquo;s like to have prosopagnosia.
This would help students feel more viscerally &amp;ldquo;what is computed and why&amp;rdquo; when it comes to facial recognition,
and it could also be used for people to better understand the condition.
Fostering empathy and understanding is always a good thing!&lt;/p&gt;
&lt;p&gt;How would we do it? With augmented reality, it would be possible to overlay or distort faces in a way that makes them unrecognizable.
The naive implementation would be to simply cover up all faces with a rectangle.
To make it a closer approximation of the prosopagnosia experience,
it would be essential to work with people with prosopagnosia.
Perhaps applying a light distortion to a face, but critically applying a different distortion to a face every time it is observed, would be a solid way to disrupt the user&amp;rsquo;s ability to recognize faces without being too distracting.
It&amp;rsquo;s difficult for me to know a priori whether this could create a convincing prosopagnosia experience,
so I will leave this snippet here, merely as food for thought, rather than pursuing the idea.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Do you think this effect could be convincing, approximating the experience of someone with prosopagnosia in someone without?&lt;/p&gt;
&lt;p&gt;In writing this snippet I Googled &amp;ldquo;prosopagnosia augmented reality&amp;rdquo; to see if anyone had tried this idea already.
Of course what I found was not my idea, but rather the reverse: using augmented reality to help those with prosopagnosia, rather than using it to help those without. That makes a lot of sense, and I feel a bit silly for it.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Maybe it could be a good class project for someone to dive deeper into this idea though. If you do, do let me know!! &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Study Group for 9.13 The Human Brain</title>
      <link>https://davidbieber.com/snippets/2021-12-05-human-brain-study-group/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-12-05-human-brain-study-group/</guid>
      
      <description>&lt;p&gt;Interested in casually learning a bit of neuroscience?
Iâ€™m gathering a group to watch / discuss 
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLUl4u3cNGP60IKRN_pFptIBxeiMc0MCJP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these lectures (MIT 9.13 The Human Brain, Spring 2019)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ping me if interested!&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how it&amp;rsquo;ll work.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll make a Discord.
Each participant will get a channel for dumping notes, thoughts, questions, whatever.
You&amp;rsquo;re free to proceed through the material at your own pace, and are encouraged to share thoughts etc. as you go.&lt;/p&gt;
&lt;p&gt;Each week I&amp;rsquo;ll check in with all participants
to see how far they&amp;rsquo;ve gotten and what their goals for the following week are.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll have (possibly asynchronous) discussions in Discord,
and every 2 weeks we&amp;rsquo;ll have a synchronous meet up (video/audio chat) with
(a) discussion questions and (b) and chance to ask questions and discuss ideas
with each other.
For these, we&amp;rsquo;ll use whenisgood to choose a time that works for the most people.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll continue for 6 weeks (3 synchronous meetups) and then reevaluate if we want to keep scheduling more.&lt;/p&gt;
&lt;p&gt;The course has 18 hours of lecture content, 12 weeks of readings, and 8 assignments.
Personally, I plan to spend about 3 hours on this material per week.
So, I&amp;rsquo;ll probably get through the video lectures but none of the other material (but I&amp;rsquo;ll adjust this plan as I go depending on what looks most interesting).
You&amp;rsquo;re welcome to do the same, or to approach the material in a different way as you see fit.&lt;/p&gt;
&lt;p&gt;Additionally, when I go to watch the lectures, I&amp;rsquo;ll post that on the Discord too. If folks are available they can join me. I encourage others to do the same, or to meet up to watch lectures in small groups. I find it can be easier to learn when watching with someone else and pausing to discuss periodically.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brains Don&#39;t Do Solomonoff Induction</title>
      <link>https://davidbieber.com/snippets/2021-01-06-brains-dont-do-solomonoff-induction/</link>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-01-06-brains-dont-do-solomonoff-induction/</guid>
      
      <description>&lt;p&gt;
&lt;a href=&#34;http://zhat.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dan Abolafia&lt;/a&gt; asked me some thought provoking questions about Solomonoff Induction and intelligence. This snippet comes from my answers.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Question: Do you think a person has a hypothesis space and prior that are fixed at the moment of birth and never change over the course of the person&amp;rsquo;s life?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;tl;dr Nope.&lt;/p&gt;
&lt;p&gt;There are different levels at which you can define hypothesis space. Consider two statements.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Statement [1]: People learn and change their prior over hypotheses based on their observations and experiences over time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In statement [1] (which I hope is unobjectionably true), &amp;ldquo;hypotheses&amp;rdquo; refers to hypotheses that will be formed during a person&amp;rsquo;s life.&lt;/p&gt;
&lt;p&gt;A person&amp;rsquo;s belief about their hypotheses can change in response to new information. This can be done in a Bayesian way, using as a prior at any particular moment their beliefs from before gaining that new information. When they gain new information, they apply Bayesian inference to compute the posterior. The posterior becomes the new prior for the next time new information is obtained.&lt;/p&gt;
&lt;p&gt;You can also &amp;ldquo;refactor&amp;rdquo; statement [1] to instead read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Statement [2]: The &amp;lsquo;computation that a person embodies&amp;rsquo; is fixed at birth.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;[1] and [2] are completely consistent with one another.&lt;/p&gt;
&lt;p&gt;(Let me know if the term &amp;lsquo;computation that a person embodies&amp;rsquo; needs clarifying.)&lt;/p&gt;
&lt;p&gt;From the perspective of [2], a person has a single prior over &amp;ldquo;hypotheses&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;(Or more precisely: a person has a single &amp;ldquo;hypothesis&amp;rdquo;, which came from a single prior. But the person didn&amp;rsquo;t &amp;lsquo;have&amp;rsquo; that prior; instead, the process by which they were created did.)&lt;/p&gt;
&lt;p&gt;Here a &amp;ldquo;hypothesis&amp;rdquo; is a much larger thing than in [1]. It&amp;rsquo;s the complete manner in which the person will determine how to respond to inputs over the course of their life. (That is, it&amp;rsquo;s the &amp;lsquo;computation that the person embodies&amp;rsquo;.)&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s an interesting question to ask whether these hypotheses come from something that approximates Solomonoff induction.&lt;/p&gt;
&lt;p&gt;Note that this wouldn&amp;rsquo;t be the &amp;ldquo;brain approximating Solomonoff induction&amp;rdquo; original question &amp;ndash; rather, it would be more like &amp;ldquo;evolution approximating Solomonoff induction&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;[Aside: And to answer this question: I don&amp;rsquo;t think single reward minimization is the most natural way to model what these complex physical processes are doing; there are lots of competing forces at play. So even though you could in principle distill them to a single reward function, I don&amp;rsquo;t think that&amp;rsquo;s the most useful way of modeling things of this complexity and scale. Update: now that I have a better understanding of Solomonoff induction, I see how this doesn&amp;rsquo;t answer the question.]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If the answer to the main question is No, there needs to be an extra-Bayesian method for choosing priors and hypothesis spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I agree with this.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s return to perspective [1]. At any given time, a person might have some beliefs about the world. Then they acquire new information. They update their beliefs. This in principle can be done in a Bayesian way all throughout ones life.&lt;/p&gt;
&lt;p&gt;In practice, there need to be other processes at work, because people develop new concepts, choose to think about some things more than others, forget stuff, etc.&lt;/p&gt;
&lt;p&gt;But with infinite memory and infinitely fast compute, an agent could in principle start with all possible concepts, and just keep doing Bayesian updates throughout their life, and I think such an agent would do quite well for themselves.&lt;/p&gt;
&lt;p&gt;Note: for a hypothetical agent like this, the answer to the original question would be Yes. But agents like this don&amp;rsquo;t exist.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Agree or disagree: The brain samples hypotheses from a universal unchanging prior.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This I disagree with. I hope perspective [1] clarifies why.&lt;/p&gt;
&lt;p&gt;Any action that the brain takes, such as sampling a hypothesis, is happening during an individual&amp;rsquo;s life. Of course it&amp;rsquo;s going to take into account all the experiences of the individual.&lt;/p&gt;
&lt;p&gt;You could of course mean &amp;ldquo;hypothesis&amp;rdquo; in the sense of [2], where the complete life experiences of the individual are inputs to the hypothesis. But then why wait until there&amp;rsquo;s a brain at all to sample the hypothesis? Now we&amp;rsquo;re talking about a hypothesis (the &amp;lsquo;computation that the person embodies&amp;rsquo;) that has been in existence at least since the person&amp;rsquo;s birth.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Do you think [1] and [2] are equivalent?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think they are two ways of describing the same situation. So, in that sense, yes.&lt;/p&gt;
&lt;p&gt;I could see why people might disagree with this though. People can make good arguments that you&amp;rsquo;re not just a fixed embodied computation. I think these arguments are wrong, but people can still make them. These hypothetical people would regard [1] as correct and [2] as incorrect, whereas I think both [1] and [2] are correct.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Bayesian inference doesn&amp;rsquo;t provide a way to create or destroy hypotheses. It merely provides a way for updating one&amp;rsquo;s beliefs about existing hypotheses. How do you reconcile this with [1] and [2]?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is right. Hypotheses can&amp;rsquo;t be &amp;ldquo;formed&amp;rdquo; from Bayesian inference. As I was describing perspective [1], I was envisioning hypotheses being formed in some (likely extra-Bayesian, but it wasn&amp;rsquo;t important) way and merely being updated via Bayesian inference.&lt;/p&gt;
&lt;p&gt;[Aside: How are these hypotheses formed though?&lt;/p&gt;
&lt;p&gt;Well, if you already have a prior for the state of the universe, then you might (1) make a determination about what variables are relevant to the situation, and (2) marginalize out all other variables. No need to do this though if memory and compute aren&amp;rsquo;t limited.]&lt;/p&gt;
&lt;p&gt;Earlier I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s return to perspective [1]. At any given time, a person might have some beliefs about the world. Then they acquire new information. They update their beliefs. This in principle can be done in a Bayesian way all throughout ones life.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How does this work, you ask?&lt;/p&gt;
&lt;p&gt;It relies on the agent starting with a prior over all possible hypotheses. This avoids the issue of needing to create hypotheses along the way, which Bayesian inference doesn&amp;rsquo;t provide a mechanism for.&lt;/p&gt;
&lt;p&gt;I tried to describe this by the following text:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With infinite memory and infinitely fast compute, an agent could in principle start with all possible concepts, and just keep doing Bayesian updates throughout their life, and I think such an agent would do quite well for themselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If I understand correctly, this &lt;strong&gt;is&lt;/strong&gt; (more or less) Solomonoff induction. (I didn&amp;rsquo;t realize this at the start of the conversation.)&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve started with a universal prior and applied bayesian inference to update it as we made observations.&lt;/p&gt;
&lt;p&gt;Of course, this isn&amp;rsquo;t what people do. So, my answer to the overall initial question is just a simple &amp;ldquo;Nope.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Cheers!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>