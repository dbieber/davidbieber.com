<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on David Bieber</title>
    <link>https://davidbieber.com/tags/math/</link>
    <description>Recent content in math on David Bieber</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://davidbieber.com/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weak and Strong Law of Large Numbers</title>
      <link>https://davidbieber.com/snippets/2021-07-18-laws-of-large-numbers/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2021-07-18-laws-of-large-numbers/</guid>
      
      <description>&lt;p&gt;$X_i$s are i.i.d random variables with finite expected value.&lt;/p&gt;
&lt;p&gt;$S_n$ is the sum of first $n$ random variables (the sample mean).&lt;br/&gt;
$\mu$ is the true mean (the population mean).&lt;/p&gt;
&lt;p&gt;Both the &lt;em&gt;Weak Law of Large Numbers&lt;/em&gt; and the &lt;em&gt;Strong Law of Large Numbers&lt;/em&gt; say that sample mean likely converges to the population mean as sample size increases.&lt;/p&gt;
&lt;p&gt;The Weak law guarantees &lt;em&gt;convergence in probability&lt;/em&gt;.&lt;br/&gt;
The Strong law guarantees &lt;em&gt;almost sure convergence&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Weak law: $\lim\limits_{n \to \infty} \text{Pr}\left(|S_n - \mu| &amp;lt; \epsilon\right) = 1$&lt;br/&gt;
Strong law: $\text{Pr}\left(\lim\limits_{n \to \infty} S_n = \mu\right) = 1$&lt;/p&gt;
&lt;p&gt;Neither occurence of &amp;ldquo;$=1$&amp;rdquo; means that any event is guaranteed. There&amp;rsquo;s simply infinitesimal probability that an event does not occur.&lt;/p&gt;
&lt;p&gt;The weak law says that for any threshold distance from the true mean $\epsilon$ that you select, as the number of samples increases, the probability that the sample mean is within $\epsilon$ from the true mean approaches one. Even as the number of samples increases there is still the possibility of the sample mean being further than $\epsilon$ from the true mean; that possibility simply approaches being infinitesimal as the number of samples increases.&lt;/p&gt;
&lt;p&gt;The strong law similarly doesn&amp;rsquo;t guarantee that the sample mean must ever equal the true mean. It says that with probability 1 (aka &amp;ldquo;almost surely&amp;rdquo;) the limit of the sample mean equals the true mean.
The reason we call probability 1 &amp;ldquo;almost surely&amp;rdquo; rather than &amp;ldquo;surely&amp;rdquo; is that there are still valid sequences of samples where the sample mean doesn&amp;rsquo;t approach the true mean; these sample outcomes are simply an infinitesimal fraction of the total sample space.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Derivative of Softmax and the Softmax Cross Entropy Loss</title>
      <link>https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/</link>
      <pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/</guid>
      
      <description>&lt;p&gt;Write $y_i = \text{softmax}(\textbf{x})_i = \frac{e^{x_i}}{\sum e^{x_d}}$.&lt;/p&gt;
&lt;p&gt;That is, $\textbf{y}$ is the softmax of $\textbf{x}$. Softmax computes a normalized exponential of its input vector.&lt;/p&gt;
&lt;p&gt;Next write $L = -\sum t_i \ln(y_i)$. This is the softmax cross entropy loss. $t_i$ is a 0/1 target representing whether the correct class is class $i$. We will compute the derivative of $L$ with respect to the inputs to the softmax function $\textbf{x}$.&lt;/p&gt;
&lt;p&gt;We have $\frac{dL}{dx_j} = -\sum t_i \frac{1}{y_i} \frac{dy_i}{d{x_j}}$ from the chain rule.&lt;/p&gt;
&lt;p&gt;We compute $\frac{dy_i}{dx_j}$ using the quotient rule.&lt;/p&gt;
&lt;p&gt;If $i = j$, this gives:&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \frac{\sum e^{x_d} \cdot e^{x_i} - e^{x_i} \cdot e^{x_i}}{(\sum e^{x_d})^2}$&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \frac{e^{x_i}}{\sum e^{x_d}} \cdot \left(\frac{\sum e^{x_d} - e^{x_i}}{\sum e^{x_d}}\right)$&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = y_i \cdot (1 - y_i)$&lt;/p&gt;
&lt;p&gt;If $i \ne j$, this gives:&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \frac{\sum e^{x_d} \cdot 0 - e^{x_i} \cdot e^{x_j}}{(\sum e^{x_d})^2}$&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = -\frac{e^{x_i}}{\sum e^{x_d}} \cdot \frac{e^{x_j}}{\sum e^{x_d}} $&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = -y_i y_j$&lt;/p&gt;
&lt;p&gt;Together these equations give us the derivative of the softmax function:&lt;/p&gt;
&lt;p&gt;$\frac{dy_i}{dx_j} = \begin{cases} y_i \cdot (1 - y_i) &amp;amp; i=j \\\ -y_i y_j &amp;amp; i \ne j \end{cases}$&lt;/p&gt;
&lt;p&gt;Using this result, we can finish computing the derivative of $L$. This gives:&lt;/p&gt;
&lt;p&gt;$\frac{dL}{dx_j} = -\sum t_i \frac{1}{y_i} \frac{dy_i}{d{x_j}} = \sum\limits_i \begin{cases} t_i (y_i - 1) &amp;amp; i=j \\\ t_i y_j &amp;amp; i \ne j \end{cases}$&lt;/p&gt;
&lt;p&gt;Since exactly one of the $t_i$s is 1 and the rest are zeros this further simplifies to:&lt;/p&gt;
&lt;p&gt;$\frac{dL}{dx_j} = y_j - t_j$&lt;/p&gt;
&lt;p&gt;We have computed the derivative of the softmax cross-entropy loss $L$ with respect to the inputs to the softmax function.&lt;/p&gt;
&lt;p&gt;This page is an experiment in publishing directly from Roam Research. &lt;del&gt;It is incomplete, and the formatting is probably all wonky. Bear with me while I get this sorted.&lt;/del&gt; Update (December 13th, 2020): The formatting looks good now!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4.2e-8 of All of Human Experience</title>
      <link>https://davidbieber.com/snippets/2020-12-03-4.2e-8-of-all-of-human-experience/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-12-03-4.2e-8-of-all-of-human-experience/</guid>
      
      <description>&lt;p&gt;About 107 billion people have &lt;em&gt;ever&lt;/em&gt; lived.&lt;/p&gt;
&lt;p&gt;Across all time, the average life expectancy of a person is just under 30 years.&lt;/p&gt;
&lt;p&gt;So, there have only been 3.21 trillion person-years experienced. Total.&lt;/p&gt;
&lt;p&gt;7.8 billion people are alive today.&lt;/p&gt;
&lt;p&gt;The average age globally is about 30 years.&lt;/p&gt;
&lt;p&gt;So living people today collectively hold about 234 billion years of experience.&lt;/p&gt;
&lt;p&gt;That means that a little &lt;strong&gt;over 7% of all human experience is held by living people&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One day spent alone constitutes 8.5e-16 of all of human experience. A month is 2.6e-14 of all of human experience.&lt;/p&gt;
&lt;p&gt;A year spent with a partner is 6.2e-13 of all of human experience.&lt;/p&gt;
&lt;p&gt;Four years spent with 1500 other students is 1.9e-9 of all of human experience.&lt;/p&gt;
&lt;p&gt;By a crude estimate, a whopping 4.2e-8 of all human experience was spent watching Game of Thrones. Humanity will never get that time back.&lt;/p&gt;
&lt;p&gt;These are small numbers, but not &lt;strong&gt;that&lt;/strong&gt; small. Thinking that my graduating class alone (not even counting the people a year above or below me) holds nearly 2e-9 of all human experience from our four years together is kind of mind-bending.&lt;/p&gt;
&lt;p&gt;For comparison, let&amp;rsquo;s imagine that the surface of Earth represents all of human experience. Then 4.2e-8 of human experience (the amount dedicated to watching Game of Thrones) is represented by 8.3 square miles.&lt;/p&gt;
&lt;p&gt;The 6.2e-13 of human experience held by you and your partner quarantining for a year? That&amp;rsquo;s over 3400 square feet. And a whole lifetime (est. 80 years) covers 3.141 acres.&lt;/p&gt;
&lt;p&gt;These are not at all negligible!&lt;/p&gt;
&lt;p&gt;All of this is to say: let&amp;rsquo;s treasure our existence, and let&amp;rsquo;s also not discount its significance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downloading the Online Encylopedia of Integer Sequences</title>
      <link>https://davidbieber.com/snippets/2020-06-28-oeis-download/</link>
      <pubDate>Sun, 28 Jun 2020 01:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-06-28-oeis-download/</guid>
      
      <description>&lt;p&gt;If you&amp;rsquo;re looking for a way to download the entire 
&lt;a href=&#34;https://oeis.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Online Encylopedia of Integer Sequences&lt;/a&gt;, you may come across advice for scraping the site, one sequence at a time. Instead of collecting the sequences one at a time, it might be sufficient to download one of their already collected archives. 
&lt;a href=&#34;https://oeis.org/stripped.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This file&lt;/a&gt; contains the integer sequences and their A-numbers, and 
&lt;a href=&#34;https://oeis.org/names.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this file&lt;/a&gt; contains the names of the sequences. No need to stress the OEIS servers when these ready-made archives are ripe for downloading!&lt;/p&gt;
&lt;p&gt;I found these files from 
&lt;a href=&#34;https://oeis.org/wiki/JSON_Format,_Compressed_Files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this page&lt;/a&gt; of the OEIS wiki. The reason I&amp;rsquo;m linking to it here is that finding that page was difficult, and I hope to save you the trouble of finding it for yourself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Future-Predicting Game Show Host</title>
      <link>https://davidbieber.com/snippets/2020-02-08-future-gameshow-host/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-02-08-future-gameshow-host/</guid>
      
        <message>A game show host reveals 2 boxes. In box A is either $1 million or $0, in box B there is $1000. You won&#39;t believe what&#39;s inside Box A!</message>
      
      <description>&lt;p&gt;A game show host reveals 2 boxes, box A and box B. In box A is either $1 million or $0, in box B there is $1000.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;re told, choose either just box A or both A and B.&lt;/p&gt;
&lt;p&gt;You keep the contents of the boxes you choose.
Also, the host knows what box you will choose, and has placed $1 million in A if you will choose only A, and $0 in A if you will choose both A and B.&lt;/p&gt;
&lt;p&gt;Some contestants, faced with this choice, would choose both A and B. They reason that since the money has already been placed, choosing A and B is guaranteed to yield $1000 more than choosing just A.
Others would choose only A, since per the host&amp;rsquo;s plans this option leads to a significantly better payout.&lt;/p&gt;
&lt;p&gt;Clearly choosing only A is $1000 worse than choosing both A and B, yet choosing A only is allegedly better given the host&amp;rsquo;s foresight. These statements appear contradictory, and this situation is known as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Newcomb%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Newcomb&amp;rsquo;s Paradox&lt;/a&gt;.
How can we reconcile these two true but contradictory statements?&lt;/p&gt;
&lt;p&gt;We do so by clarifying:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Right now&lt;/em&gt; choosing only A is $1000 worse than choosing both A and B.&lt;/li&gt;
&lt;li&gt;However, having a brain&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;em&gt;at the time the money is placed&lt;/em&gt; that will lead you to choose only A at the time of choosing gives the $1 million outcome.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is this something you have control over? Can you &lt;em&gt;decide&lt;/em&gt; at the time the money is placed to have a brain that will lead you to later choose only box A?&lt;/p&gt;
&lt;p&gt;Yes.&lt;/p&gt;
&lt;p&gt;It may seem like &lt;em&gt;no&lt;/em&gt;, for a few reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You didn&amp;rsquo;t even know there was a choice to make.&lt;/li&gt;
&lt;li&gt;You surely don&amp;rsquo;t get to make choices about how your brain works, right?&lt;/li&gt;
&lt;li&gt;Since the host is able know your choice ahead of time, that must mean it&amp;rsquo;s predetermined, right?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, none of these reasons are entirely valid.&lt;/p&gt;
&lt;p&gt;First, it&amp;rsquo;s true that at the time the money is placed, you didn&amp;rsquo;t necessarily know you had a choice to make. You hadn&amp;rsquo;t yet been informed that you would later be on this game show. However, it&amp;rsquo;s possible you considered the situation as a hypothetical anyway (maybe while reading a blog post, for instance). It isn&amp;rsquo;t necessary that you considered precisely the situation that you would later find yourself in, only that you&amp;rsquo;ve given conscious deliberate thought to the way you make choices. By thinking this through, you can decide now whether you have a brain that would later choose box A or one that would later choose both A and B, should the game show situation ever arise.&lt;/p&gt;
&lt;p&gt;Next, you absolutely do get to make choices about how your brain works. You can choose when to sleep, when to study, what substances to ingest, and most importantly, what to focus your attention on. All of these levers are yours to pull, and they all affect how your brain will operate in the future.&lt;/p&gt;
&lt;p&gt;At the time you choose the boxes, it&amp;rsquo;s still your decision as to which box(es) to choose. (In the sense that it&amp;rsquo;s your brain that produces the decision.)
But you must have been predictable, at least to the host, for them to have predicted what choice you would later make.
(Nothing wrong with being predictable!)&lt;/p&gt;
&lt;p&gt;How is it that you were predictable? Does this require determinism?
One extreme way you may have been predictable is if the host knows exactly how events will play out, right down to the thoughts you will have and the choices you will make.&lt;/p&gt;
&lt;p&gt;It doesn&amp;rsquo;t have to be &lt;em&gt;quite&lt;/em&gt; so extreme though. The host only claimed to have roughly one bit of information about you and the future: what binary choice you would make when confronted with the box dilemma. That doesn&amp;rsquo;t require omniscience. The host may have no idea what the weather is going to be the next day, as long as they&amp;rsquo;re able to predict whether or not you&amp;rsquo;re taking box B.&lt;/p&gt;
&lt;p&gt;So maybe they know a little about how your mind works, or they&amp;rsquo;ve read something you&amp;rsquo;ve posted online that lets them conclude how you will handle the box situation.&lt;/p&gt;
&lt;p&gt;You must consider that whatever they know, they knew it at the time that they chose how to fill the boxes. (They likely also knew it at the time they chose to host the game show in the first place.)&lt;/p&gt;
&lt;p&gt;What I think is most likely is that this host does know precisely how you think about this choice. They do this by understanding to a very high degree of precision how your brain is going to cope with the decision of which boxes to choose. Think of the host as a very skilled poker player, that can read you like a book. Instead of just knowing whether you&amp;rsquo;re bluffing or not, they know what you&amp;rsquo;re going to think 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/6640273&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;before you even think it&lt;/a&gt;. It&amp;rsquo;s like weather forecasting, but instead of understanding the atmosphere, they&amp;rsquo;re understanding you.&lt;/p&gt;
&lt;p&gt;And like with weather forecasting, their model is imperfect. It&amp;rsquo;s good enough to know what box you&amp;rsquo;ll choose, with certainty, but only because that only requires forecasting a small window (a week, let&amp;rsquo;s say) into the future. Like the atmospheric conditions that determine the weather, your brain is a chaotic system. Any imperfection in the model, however small, will eventually blow up and lead to large errors in the model in the future. So our host can&amp;rsquo;t model your every thought a year into the future, let alone e.g. the whole population&amp;rsquo;s behaviors.&lt;/p&gt;
&lt;p&gt;So what boxes should you choose?&lt;/p&gt;
&lt;p&gt;You should choose to open just box A. By reaching this conclusion (and more importantly, by adopting the strategy for reasoning that leads to this conclusion), you guarantee yourself $1 million the next time you encounter a wealthy generous time traveling game show host interested in philosophy and mathematics.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I say &amp;ldquo;brain&amp;rdquo;, but I really mean the whole of your situation and life circumstances. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>What does &#34;Ordinary&#34; mean in Ordinary Least Squares?</title>
      <link>https://davidbieber.com/snippets/2020-01-08-ordinary-least-squares/</link>
      <pubDate>Wed, 08 Jan 2020 02:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/snippets/2020-01-08-ordinary-least-squares/</guid>
      
        <message>It means the standard method. Nothing fancy.</message>
      
      <description>&lt;p&gt;&amp;ldquo;Ordinary Least Squares&amp;rdquo; regression, or OLS for short, is a method for finding a best-fit line, given a set of data points. When people refer to linear regression without additional context or qualifiers, they&amp;rsquo;re probably referring to ordinary least squares regression.&lt;/p&gt;
&lt;p&gt;So, what does &amp;ldquo;ordinary&amp;rdquo; mean in this context? Is it a mathematical term with a precise technical meaning?&lt;/p&gt;
&lt;p&gt;Well, no. In this case &amp;ldquo;ordinary&amp;rdquo; means exactly what you might expect: standard, vanilla, lacking any bells and whistles. Since least squares regression is so common, there are lots of variants. Weighted Least Squares (in which each data point is given its own weight) is a such a variant. So, in OLS, the O for ordinary simply means we&amp;rsquo;re not referring to one of these variants.&lt;/p&gt;
&lt;p&gt;What about &amp;ldquo;ordinary differential equations&amp;rdquo; (ODEs)? What does ordinary refer to there?&lt;/p&gt;
&lt;p&gt;In that context, ordinary refers to ordinary derivatives, as opposed to partial derivatives. So again, it&amp;rsquo;s not a mathematical term like a reference to ordinal numbers.&lt;/p&gt;
&lt;p&gt;In both cases, ordinary seems to take on its ordinary English definition &amp;ndash; its OED, if I may.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Weisfeiler-Lehman Isomorphism Test</title>
      <link>https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/</guid>
      
      <description>&lt;h3 id=&#34;graph-isomorphism&#34;&gt;Graph Isomorphism&lt;/h3&gt;
&lt;p&gt;Two graphs are considered isomorphic if there is a mapping between the nodes of the graphs that preserves node adjacencies.
That is, a pair of nodes may be connected by an edge in the first graph if and only if the corresponding pair of nodes in the second graph is also connected by an edge in the same way.
An example of two isomorphic graphs is shown here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism.png&#34; alt=&#34;Example of isomorphic graphs&#34; title=&#34;Two isomorphic graphs are shown.&#34;&gt;
&lt;em&gt;Figure: Graph 1 and Graph 2 are isomorphic.
The correspondance between nodes is illustrated by the node colors and numbers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In general, determining whether two graphs are isomorphic when the correspondance is not provided is a challenging problem; precisely how hard this problem is remains an open question in computer science.
It isn&amp;rsquo;t known whether there is a polynomial time algorithm for determining whether graphs are isomorphic, and it also isn&amp;rsquo;t known whether the problem is $\text{NP-complete}.$
The graph isomorphism problem may even be an example of an 
&lt;a href=&#34;https://en.wikipedia.org/wiki/NP-intermediate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$\text{NP-intermediate}$&lt;/a&gt; problem, but this would only be possible if $\text{P} \ne \text{NP}.$&lt;/p&gt;
&lt;h3 id=&#34;the-weisfeiler-lehman-isomorphism-test&#34;&gt;The Weisfeiler-Lehman Isomorphism Test&lt;/h3&gt;
&lt;p&gt;Here is the algorithm for the Weisfeiler-Lehman Isomorphism Test.
It produces for each graph a canonical form.
If the canonical forms of two graphs are not equivalent, then the graphs are definitively not isomorphic.
However, it is possible for two non-isomorphic graphs to share a canonical form, so this test alone cannot provide conclusive evidence that two graphs are isomorphic.&lt;/p&gt;
&lt;p&gt;The Algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For iteration $i$ of the algorithm we will be assigning to each node a tuple $L_{i,n}$ containing the node&amp;rsquo;s old compressed label and a multiset of the node&amp;rsquo;s neighbors&#39; compressed labels.
A multiset is a set (a collection of elements where order is not important) where elements may appear multiple times.&lt;/li&gt;
&lt;li&gt;At each iteration we will additionally be assigning to each node $n$ a new &amp;ldquo;compressed&amp;rdquo; label $C_{i,n}$ for that node&amp;rsquo;s set of labels.
Any two nodes with the same $L_{i,n}$ will get the same compressed label.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;To begin, we initialize $C_{0,n} = 1$ for all nodes $n$.&lt;/li&gt;
&lt;li&gt;At iteration $i$ of the algorithm (beginning with $i=1$), for each node $n$, we set $L_{i,n}$ to be a tuple containing the node&amp;rsquo;s old label $C_{i-1,n}$ and the multiset of compressed node labels $C_{i-1,m}$ from all nodes $m$ neighboring $n$ from the previous iteration $(i-1)$.&lt;/li&gt;
&lt;li&gt;We then complete iteration $i$ by setting $C_{i,n}$ to be a new &amp;ldquo;compressed&amp;rdquo; label, such as a hash of $L_{i,n}$.
Any two nodes with the same labels $L_{i,n}$ must get the same compressed label $C_{i,n}$.&lt;/li&gt;
&lt;li&gt;Partition the nodes in the graph by their compressed label.
Repeat 2 + 3 for up to $N$ (the number of nodes) iterations, or until there is no change in the partition of nodes by compressed label from one iteration to the next.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When using this method to determine graph isomorphism, it may be applied in parallel to the two graphs.
The algorithm may be terminated early after iteration $i$ if the sizes of partitions of nodes partitioned by compressed labels diverges between the two graphs; if this is the case, the graphs are not isomorphic.&lt;/p&gt;
&lt;h3 id=&#34;example-of-the-weisfeiler-lehman-isomorphism-test&#34;&gt;Example of the Weisfeiler-Lehman Isomorphism Test&lt;/h3&gt;
&lt;p&gt;We demonstrate here the Weisfeiler-Lehman isomorphism test using the example graphs from above.
The graphs are shown again here for completeness.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-000.png&#34; alt=&#34;Two isomorphic graphs are shown.&#34; title=&#34;Two isomorphic graphs are shown.&#34;&gt;
&lt;em&gt;Figure: Graph 1 and Graph 2 are isomorphic.
We will apply the Weisfeiler-Lehman isomorphism test to these graphs as a means of illustrating the test.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To initialize the algorithm (Step 1), we set $C_{0,n} = 1$ for all nodes $n$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-001.png&#34; alt=&#34;Initialization: $C\_{0,n} = 1$ for all nodes $n$&#34; title=&#34;We initialize $C_{0,n} = 1$ for all nodes.&#34;&gt;&lt;/p&gt;
&lt;p&gt;For iteration 1, Step 2, we compute $L_{1,n}$.
The first part of a node&amp;rsquo;s $L$ is the node&amp;rsquo;s old compressed label; the second part of a node&amp;rsquo;s $L$ is the multiset of the neighboring nodes&#39; compressed labels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-002.png&#34; alt=&#34;Iteration 1, Step 2: $L\_{1,n}$&#34; title=&#34;Iteration 1, Step 2: $L_{1,n}$&#34;&gt;&lt;/p&gt;
&lt;p&gt;For iteration 1, Step 3, we introduce &amp;ldquo;compressed&amp;rdquo; labels $C_{1,n}$ for the nodes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-003.png&#34; alt=&#34;Iteration 1, Step 3: $C\_{1,n}$&#34; title=&#34;Iteration 1, Step 3: $C_{1,n}$&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now begin iteration 2.
In iteration 2, Step 2, we compute $L_{2,n}$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-004.png&#34; alt=&#34;Iteration 2, Step 2: $L\_{2,n}$&#34; title=&#34;Iteration 2, Step 2: $L_{2,n}$&#34;&gt;&lt;/p&gt;
&lt;p&gt;In iteration 2, Step 3, we compute $C_{2,n}$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-005.png&#34; alt=&#34;Iteration 2, Step 3: $C\_{2,n}$&#34; title=&#34;Iteration 2, Step 3: $C_{2,n}$&#34;&gt;&lt;/p&gt;
&lt;p&gt;In iteration 3, Step 2, we compute $L_{3,n}$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-006.png&#34; alt=&#34;Iteration 3, Step 2: $L\_{3,n}$&#34; title=&#34;Iteration 3, Step 2: $L_{3,n}$&#34;&gt;&lt;/p&gt;
&lt;p&gt;In iteration 3, Step 3, we compute $C_{3,n}$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-isomorphism-007.png&#34; alt=&#34;Iteration 3, Step 3: $C\_{3,n}$&#34; title=&#34;Iteration 3, Step 3: $C_{3,n}$&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the partition of nodes by compressed label has not changed from $C_{2,n}$ to $C_{3,n}$, we may terminate the algorithm here.&lt;/p&gt;
&lt;p&gt;Concretely, the partition of nodes by compressed label may be represented as the number of nodes with each compressed label.
That is: &lt;strong&gt;&amp;ldquo;2 7s, 1 8, and 2 9s&amp;rdquo;&lt;/strong&gt;.
This is the canonical form of our graph.
Since both Graph 1 and Graph 2 have this same canonical form, we cannot rule out the possibility that they are isomorphic (they &lt;em&gt;are&lt;/em&gt; in fact isomorphic, but the algorithm doesn&amp;rsquo;t allow us to conclude this definitively.)&lt;/p&gt;
&lt;h3 id=&#34;implementation-considerations&#34;&gt;Implementation Considerations&lt;/h3&gt;
&lt;p&gt;One detail omitted is that the algorithm requires canonical forms of the multisets for comparison.
Placing the elements of the multiset in sorted order provides one way of doing this.&lt;/p&gt;
&lt;p&gt;Additionally, for the representation of the graph that the algorithm emits to be deterministic, we need to establish a convention for creating compressed labels.
One possible convention is to use increasing integers starting from 1, and to assign compressed labels to nodes in lexicographical order of their non-compressed labels.
Another possible convention is to use the hash of the multiset to create the compressed label.&lt;/p&gt;
&lt;p&gt;When comparing the partition of the nodes by compressed label from one iteration to the next to see whether to proceed to another iteration, it is sufficient to compare which nodes are in each partition.
If the partitions change, we proceed to the next iteration.
If no partition changes, the algorithm can terminate early.
When comparing the partitions of the nodes across graphs to see if graphs are isomorphic, we use the canonical form as described above in the algorithm.&lt;/p&gt;
&lt;h3 id=&#34;finding-the-correspondance-between-isomorphic-graphs&#34;&gt;Finding the Correspondance Between Isomorphic Graphs&lt;/h3&gt;
&lt;p&gt;The core idea of the Weisfeiler-Lehman isomorphism test is to find for each node in each graph a signature based on the neighborhood around the node.
These signatures can then be used to find the correspondance between nodes in the two graphs, which can be used to check for isomorphism.&lt;/p&gt;
&lt;p&gt;In the algorithm descibed above, the &amp;ldquo;compressed labels&amp;rdquo; serve as the signatures.
Since multiple nodes may have the same compressed label, there are multiple possible correspondances suggested by a Weisfeiler-Lehman labeling.
The Weisfeiler-Lehman isomorphism test itself does not provide a way of narrowing down the possible correspondances further.&lt;/p&gt;
&lt;h3 id=&#34;the-disappearance-of-boris-weisfeiler&#34;&gt;The Disappearance of Boris Weisfeiler&lt;/h3&gt;
&lt;p&gt;Boris Weisfeiler &amp;ndash; one of the two mathematicians responsible for the Weisfeiler-Lehman isomorphism test &amp;ndash; was a professor at Penn State University until he disappeared mysteriously in Chile in 1985.
His disappearance in the 80s is unsettling, and I would encourage you to visit 
&lt;a href=&#34;http://www.boris.weisfeiler.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;boris.weisfeiler.com&lt;/a&gt; to learn more about the search for Weisfeiler and the ongoing (2018) judicial proceedings surrounding his disappearance.&lt;/p&gt;
&lt;p&gt;Steven List produced an award winning short film called &lt;em&gt;The Colony&lt;/em&gt; based on the disappearance of Boris Weisfeiler, available 
&lt;a href=&#34;https://vimeo.com/60220290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;on Vimeo here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After a brief internet search, I have been unable to find any information about Weisfeiler&amp;rsquo;s colleague and coauthor A. Lehman, not even a first name.
(Update Aug. 2020: A reader pointed me to 
&lt;a href=&#34;https://towardsdatascience.com/a-forgotten-story-of-soviet-ai-4af5daaf9cdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this recent story&lt;/a&gt; about &lt;em&gt;Andrey&lt;/em&gt; Leman. Thank you!)&lt;/p&gt;
&lt;h3 id=&#34;np-intermediate-problems&#34;&gt;NP-Intermediate Problems&lt;/h3&gt;
&lt;p&gt;As mentioned in the introduction, it is not known whether the graph isomorphism problem is in $\text{P}$ or $\text{NP-complete}$.
In fact, it is not even known whether graph isomorphism falls in either of these categories, or whether this problem is $\text{NP-intermediate}$.
The problem of factoring integers is another problem like this, where it is unknown whether the problem is in $\text{P}$, $\text{NP-complete}$, or is $\text{NP-intermediate}$.
Bear in mind that $\text{NP-intermediate}$ problems can only exist if $\text{P} \ne \text{NP}$.
So if anyone were to show conclusively that factoring or graph isomorphism were $\text{NP-intermediate}$ they would also have shown $\text{P} \ne \text{NP}$ and won the 
&lt;a href=&#34;https://www.claymath.org/millennium-problems/p-vs-np-problem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clay Institute P vs NP Millenium Problem&lt;/a&gt;&amp;rsquo;s million dollar prize.&lt;/p&gt;
&lt;h3 id=&#34;applications-of-graph-isomorphisms&#34;&gt;Applications of Graph Isomorphisms&lt;/h3&gt;
&lt;p&gt;Now that you&amp;rsquo;re familiar with the Weisfeiler-Lehman test for graph isomorphisms, what is the graph isomorphism problem good for?&lt;/p&gt;
&lt;p&gt;There are applications of the graph isomorphism problem in, for example, computational chemistry and in circuit design.&lt;/p&gt;
&lt;p&gt;In chemistry, it is common to represent a molecule as a graph, with the atoms in the molecule being the nodes of the graph and the bonds between molecules being the edges.
Here, the graph isomorphism problem can be used to determine if two chemical structures are in fact the same structure.
This is important for 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Drug_discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;drug discovery&lt;/a&gt;, where scientists are working to create molecular structures that will be useful in order to fight diseases.&lt;/p&gt;
&lt;p&gt;A circuit is commonly represented as a graph as well.
The components in the circuit form the nodes of the graph, and the connections between components form the edges.
The graph isomorphism problem is useful here for determining whether two circuits that are laid out different are in fact identical.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Weisfeiler and Lehman first proposed this method in their paper &lt;em&gt;A reduction of a graph to a canonical form and an algebra arising during this reduction&lt;/em&gt; in 1968.
An English translation of this paper, originally published in Russian, is 
&lt;a href=&#34;https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More recently the method is also described succinctly in 
&lt;a href=&#34;http://www.jmlr.org/papers/volume12/shervashidze11a/shervashidze11a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weisfeiler-Lehman Graph Kernels (Shervashidze 2011)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now go forth and detect graph isomorphisms!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>